{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37be34fc-0fa9-4811-865c-a3fdc38d38e8",
   "metadata": {},
   "source": [
    "# Fine-tune TinyLlama-1.1B for text-to-SQL generation\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this workshop module, you will learn how to fine-tune a Llama-based LLM ([TinyLlama-1.1B-Chat-v1.0](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0)) using causal language modelling so that the model learns how to generate SQL queries for text-based instructions. Your fine-tuning job will be launched using SageMaker Training which provides a serverless training environment where you do not need to manage the underlying infrastructure. You will learn how to configure a PyTorch training job using [SageMaker's PyTorch estimator](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html), and how to leverage the [Hugging Face Optimum Neuron](https://github.com/huggingface/optimum-neuron) package to easily run the PyTorch training job with AWS Trainium accelerators via an [AWS EC2 trn1.2xlarge instance](https://aws.amazon.com/ec2/instance-types/trn1/).\n",
    "\n",
    "For this module, you will be using the [b-mc2/sql-create-context](https://huggingface.co/datasets/b-mc2/sql-create-context) dataset which consists of thousands of examples of SQL schemas, questions about the schemas, and SQL queries intended to answer the questions.\n",
    "\n",
    "*Dataset example 1:*\n",
    "* *SQL schema/context:* `CREATE TABLE management (department_id VARCHAR); CREATE TABLE department (department_id VARCHAR)`\n",
    "* *Question:* `How many departments are led by heads who are not mentioned?`\n",
    "* *SQL query/answer:* `SELECT COUNT(*) FROM department WHERE NOT department_id IN (SELECT department_id FROM management)`\n",
    "\n",
    "*Dataset example 2:*\n",
    "* *SQL schema/context:* `CREATE TABLE courses (course_name VARCHAR, course_id VARCHAR); CREATE TABLE student_course_registrations (student_id VARCHAR, course_id VARCHAR)`\n",
    "* *Question:* `What are the ids of all students for courses and what are the names of those courses?`\n",
    "* *SQL query/answer:* `SELECT T1.student_id, T2.course_name FROM student_course_registrations AS T1 JOIN courses AS T2 ON T1.course_id = T2.course_id`\n",
    "\n",
    "By fine-tuning the model over several thousand of these text-to-SQL examples, the model will then learn how to generate an appropriate SQL query when presented with a SQL context and a free-form question.\n",
    "\n",
    "This text-to-SQL use case was selected so you can successfully fine-tune your model in a reasonably short amount of time (~20 minutes) which is appropriate for this 1hr workshop. Although this is a relatively simple use case, please keep in mind that the same techniques and components used in this module can also be applied to fine-tune LLMs for more advanced use cases such as writing code, summarizing documents, creating blog posts - the possibilities are endless!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866074ee-c300-4793-8e63-adbcfc314ad8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prerequisites\n",
    "\n",
    "This notebook uses the SageMaker Python SDK to prepare, launch, and monitor the progress of a PyTorch-based training job. Before we get started, it is important to upgrade the SageMaker SDK to ensure that you are using the latest version. Run the next two cells to upgrade the SageMaker SDK and set up your session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3264aae2-1f18-4b59-a92c-2f169903c202",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m  DEPRECATION: Building 's3' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 's3'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  DEPRECATION: Building 'futures' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'futures'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Upgrade SageMaker SDK to the latest version\n",
    "%pip install -U sagemaker awscli huggingface_hub ipywidgets s3 -q 2>&1 | grep -v \"warnings/venv\"\n",
    "# Definitely restart your kernel after this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b5ed574-6db5-471b-8515-c0f6189e653e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "sagemaker_config_logger = logging.getLogger(\"sagemaker.config\")\n",
    "sagemaker_config_logger.setLevel(logging.WARNING)\n",
    "\n",
    "# Import SageMaker SDK, setup our session\n",
    "from sagemaker import get_execution_role, Session\n",
    "from sagemaker.pytorch import PyTorch\n",
    "import boto3\n",
    "\n",
    "region_name=\"us-west-2\" #this is hard coded to a specific region because of Workshop quotas.  You could use sess.boto_region_name\n",
    "sess = Session(boto_session=boto3.Session(region_name=region_name))\n",
    "default_bucket = sess.default_bucket()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce630d1",
   "metadata": {},
   "source": [
    "This next command just configures the EC2 instance (in us-west-2) to have a default region of us-east-2.  This is specific to the environment in AWS Workshop Studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5542b3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws configure set region us-west-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3a1f57",
   "metadata": {},
   "source": [
    "## Log into Hugging Face\n",
    "\n",
    "The following step is recommended but optional.  If you can log in with your Hugging Face token, it will let you avoid any rate limits for unauthenticated requests.  Even though none of the models or datasets we are using require special permission, if you don't log in your training may fail because of too many unauthenticated requests.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f142253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the cell below stays empty, RESTART YOUR KERNEL if you didn't and run the cells above again\n",
    "# If you can't login in, you can proceed to the next cell.\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# Uncheck \"Add token as git credential\" or just ignore the error message about it not being added.\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4193108b-25fb-4d3e-85db-c66b8c04c251",
   "metadata": {},
   "source": [
    "## Specify the Optimum Neuron deep learning container (DLC) image\n",
    "\n",
    "The SageMaker Training service uses containers to execute your training script, allowing you to fully customize your training script environment and any required dependencies. For this workshop, you will use a recent Pytorch Training deep learning container (DLC) image which is an AWS-maintained image containing the Neuron SDK and PyTorch.  The Optimum-Neuron library is installed with the requirements.txt file in the assets directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "247ad886-6977-4295-947b-86d4892b48bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training-neuronx:2.7.0-neuronx-py310-sdk2.24.1-ubuntu22.04\n"
     ]
    }
   ],
   "source": [
    "# Specify the Neuron DLC that we will use for training\n",
    "#   For now, we'll use the standard Neuron DLC and install Optimum Neuron v0.0.27 at training time because we want to use a later SDK \n",
    "#   You can see more about the images here: https://github.com/aws-neuron/deep-learning-containers?tab=readme-ov-file#pytorch-training-neuronx\n",
    "\n",
    "training_image = f\"763104351884.dkr.ecr.{sess.boto_region_name}.amazonaws.com/pytorch-training-neuronx:2.7.0-neuronx-py310-sdk2.24.1-ubuntu22.04\"\n",
    "print(training_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8802bc-657a-419d-b86d-eb8af5eff90e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Configure the PyTorch Estimator\n",
    "\n",
    "The SageMaker SDK includes a [PyTorch Estimator](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html) class which you can use to define a PyTorch training job that will be executed in the SageMaker managed environment. \n",
    "\n",
    "In the following cell, you will create a PyTorch Estimator which will run the attached `finetune_model.py` training script on an ml.trn1.2xlarge instance. The `finetune_model.py` script is an Optimum Neuron training script that can be used for causal language modelling with AWS Trainium. The scripts will be downloaded as the instance is brought up, and the scripts will download the model and the datasets onto the SageMaker training instance.\n",
    "\n",
    "The PyTorch Estimator has many parameters that can be used to configure your training job. A few of the most important parameters include:\n",
    "\n",
    "- *entry_point*: refers to the name of the training script that will be executed as part of this training job\n",
    "- *source_dir*: the path to the local source code directory (relative to your notebook) that will be packaged up and included inside your training container\n",
    "- *instance_count*: defines how many EC2 instances to use for this training job\n",
    "- *instance_type*: determines which type of EC2 instance will be used for training\n",
    "- *image_uri*: defines which training DLC will be used to run the training job (see Neuron DLC, above)\n",
    "- *distribution*: determines which type of distribution to use for the training job - you will need 'torch_distributed' for this workshop\n",
    "- *environment*: provides a dictionary of environment variables which will be applied to your training environment\n",
    "- *hyperparameters*: provides a dictionary of command-line arguments to pass to your training script, ex: finetune_model.py\n",
    "\n",
    "In the `hyperparameters` section, you can see the specific command-line arguments that are used to control the behavior of the `finetune_model.py` training script. Notably:\n",
    "- *model_id*: specifies which model you will be fine-tuning, in this case a recent checkpoint from the TinyLlama-1.1B project\n",
    "- *tokenizer_id*: specifies which tokenizer you will used to tokenize the dataset examples during training\n",
    "- *output_dir*: directory in which the fine-tuned model will be saved. Here we use the SageMaker-specific `/opt/ml/model` directory. At the end of the training job, SageMaker automatically copies the contents of this directory to the output S3 bucket\n",
    "- *tensor_parallel_size*: the tensor parallel degree for which we want to use for training. In this case we use '2' to shard the model across the 2 NeuronCores available in the trn1.2xlarge instance\n",
    "- *bf16*: request BFloat16 training\n",
    "- *per_device_train_batch_size*: the microbatch size to be used for fine-tuning\n",
    "- *gradient_accumulation_steps*: how many steps for which gradients will be accumulated between updates\n",
    "- *max_steps*: the maximum number of steps of fine-tuning that we want to perform\n",
    "- *lora_r*, *lora_alpha*, *lora_dropout*: the LoRA rank, alpha, and dropout values to use during fine-tuning\n",
    "\n",
    "The below estimator has been pre-configured for you, so you do not need to make any changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7dd1c2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the hyperparameters are command-line args passed to the finetune_model.py script to control its behavior\n",
    "# Create hyperparameters dictionary\n",
    "hyperparameters = {\n",
    "    \"model_id\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    \"tokenizer_id\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    \"skip_cache_push\": True,\n",
    "    \"output_dir\": \"/opt/ml/model\",\n",
    "    \"tensor_parallel_size\": 2,\n",
    "    \"bf16\": True,\n",
    "    \"per_device_train_batch_size\": 2,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"max_steps\": 1000,\n",
    "    \"lora_r\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"logging_steps\": 10,\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"dataloader_drop_last\": True,\n",
    "    \"disable_tqdm\": True,\n",
    "}\n",
    "\n",
    "# Set up environment variables\n",
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "token = api.token\n",
    "environment = {\"FI_EFA_FORK_SAFE\": \"1\", \"WANDB_DISABLED\": \"true\"}\n",
    "\n",
    "# from huggingface_hub import HfFolder\n",
    "# token = HfFolder.get_token()\n",
    "# if token is not None:\n",
    "#     environment[\"HF_TOKEN\"] = token\n",
    "\n",
    "# Set up the PyTorch estimator\n",
    "pt_estimator = PyTorch(\n",
    "    entry_point=\"finetune_model.py\",\n",
    "    source_dir=\"./assets\",\n",
    "    role=get_execution_role(),\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.trn1.2xlarge\",\n",
    "    disable_profiler=True,\n",
    "    output_path=f\"s3://{default_bucket}/neuron_events2025_fhirfly\",\n",
    "    base_job_name=\"trn1-tinyllama\",\n",
    "    sagemaker_session=sess,\n",
    "    code_bucket=f\"s3://{default_bucket}/neuron_events2025_fhirfly_code\",\n",
    "    checkpoint_s3_uri=f\"s3://{default_bucket}/neuron_events2025_fhirfly_output\",\n",
    "    image_uri=training_image,\n",
    "    distribution={\"torch_distributed\": {\"enabled\": True}},\n",
    "    environment=environment,\n",
    "    disable_output_compression=True,\n",
    "    hyperparameters=hyperparameters\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2278940b-f563-4582-9df0-bd56d9b5fd28",
   "metadata": {},
   "source": [
    "## Launch the training job\n",
    "\n",
    "Once the estimator has been created, you can then launch your training job by calling `.fit()` on the estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "65d46fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_s3(s3_uri: str):\n",
    "    import pandas as pd\n",
    "    from io import StringIO\n",
    "    from datasets import Dataset\n",
    "    \n",
    "    parts = s3_uri.replace(\"s3://\", \"\").split(\"/\", 1)\n",
    "    bucket, key = parts[0], parts[1]\n",
    "\n",
    "    # Initialize S3 client\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    \n",
    "    obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "    csv_data = obj[\"Body\"].read().decode(\"utf-8\")\n",
    "\n",
    "    # Load into pandas DataFrame\n",
    "    df = pd.read_csv(StringIO(csv_data))\n",
    "    print(f\"Loaded {len(df)} rows from {s3_uri}\")\n",
    "\n",
    "    # Convert to Hugging Face Dataset\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "785b3b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 160 rows from s3://sagemaker-us-west-2-526909565990/neuron_events2025_fhirfly/data/clinical_notes_with_pii_added.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Unnamed: 0', 'patient_id', 'note', 'question', 'answer', 'task', 'note_title', 'diagnosis', 'note_with_pii'],\n",
       "    num_rows: 160\n",
       "})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_uri = 's3://sagemaker-us-west-2-526909565990/neuron_events2025_fhirfly/data/clinical_notes_with_pii_added.csv'\n",
    "load_dataset_s3(s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b7829c64-0190-43c3-be1a-0ccce7d45248",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.telemetry.telemetry_logging:SageMaker Python SDK will collect telemetry to help us better understand our user's needs, diagnose issues, and deliver additional features.\n",
      "To opt out of telemetry, please disable via TelemetryOptOut parameter in SDK defaults config. For more information, refer to https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk.\n",
      "INFO:sagemaker:Creating training-job with name: trn1-tinyllama-2025-11-09-00-50-11-469\n"
     ]
    }
   ],
   "source": [
    "# Call fit() on the estimator to initiate the training job\n",
    "pt_estimator.fit({'train': s3_uri}, wait=False, logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77434b2-94d7-4256-8d0b-d5d2ddb1d5ae",
   "metadata": {},
   "source": [
    "## Monitor the training job\n",
    "\n",
    "When the training job has been launched, the SageMaker Training service will then take care of:\n",
    "- launching and configuring the requested EC2 infrastructure for your training job\n",
    "- launching the requested container image on each of the EC2 instances\n",
    "- copying your source code directory and running your training script within the container(s)\n",
    "- storing your trained model artifacts in Amazon Simple Storage Service (S3)\n",
    "- decommissioning the training infrastructure\n",
    "\n",
    "While the training job is running, the following cell will periodically check and output the job status. When you see 'Completed', you know that your training job is finished and you can proceed to the remainder of the notebook. The training job typically takes about 20 minutes to complete.\n",
    "\n",
    "If you are interested in viewing the output logs from your training job, you can view the logs by navigating to the AWS CloudWatch console, selecting `Logs -> Log Groups` in the left-hand menu, and then looking for your SageMaker training job in the list. **Note:** it will usually take 4-5 minutes before the infrastructure is running and the output logs begin to be populated in CloudWatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c223037-2f8e-4eb0-9e4b-ff4dac6ede7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-09T00:50:14.526139 Training job status: InProgress!\n",
      "2025-11-09T00:50:44.593382 Training job status: InProgress!\n",
      "2025-11-09T00:51:14.683317 Training job status: InProgress!\n",
      "2025-11-09T00:51:44.749667 Training job status: InProgress!\n",
      "2025-11-09T00:52:14.814258 Training job status: InProgress!\n",
      "2025-11-09T00:52:44.878908 Training job status: InProgress!\n",
      "2025-11-09T00:53:14.947276 Training job status: InProgress!\n",
      "2025-11-09T00:53:45.012044 Training job status: InProgress!\n",
      "2025-11-09T00:54:15.087948 Training job status: InProgress!\n",
      "2025-11-09T00:54:45.199883 Training job status: InProgress!\n",
      "2025-11-09T00:55:15.274025 Training job status: InProgress!\n",
      "2025-11-09T00:55:45.341262 Training job status: InProgress!\n",
      "2025-11-09T00:56:15.408613 Training job status: InProgress!\n",
      "2025-11-09T00:56:45.474569 Training job status: InProgress!\n",
      "2025-11-09T00:57:15.544323 Training job status: InProgress!\n",
      "2025-11-09T00:57:45.606887 Training job status: InProgress!\n",
      "2025-11-09T00:58:15.668417 Training job status: InProgress!\n",
      "2025-11-09T00:58:45.738130 Training job status: InProgress!\n",
      "2025-11-09T00:59:15.873660 Training job status: InProgress!\n",
      "2025-11-09T00:59:45.942287 Training job status: InProgress!\n"
     ]
    }
   ],
   "source": [
    "# Periodically check job status until it shows 'Completed' (ETA ~20 minutes)\n",
    "#  You can also monitor job status in the SageMaker console, and view the\n",
    "#  SageMaker Training job logs in the CloudWatch console\n",
    "from time import sleep\n",
    "from datetime import datetime\n",
    "\n",
    "while (job_status := pt_estimator.jobs[-1].describe()['TrainingJobStatus']) not in ['Completed', 'Error', 'Failed']:\n",
    "    print(f\"{datetime.now().isoformat()} Training job status: {job_status}!\")\n",
    "    sleep(30)\n",
    "\n",
    "print(f\"\\n{datetime.now().isoformat()} Training job status: {job_status}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c94343-b0c6-4903-82cc-c8ab2f88b26b",
   "metadata": {},
   "source": [
    "'## Determine location of fine-tuned model artifacts\n",
    "\n",
    "Once the training job has completed, SageMaker will copy your fine-tuned model artifacts to a specified location in S3.\n",
    "\n",
    "In the following cell, you can see how to programmatically determine the location of your model artifacts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "213af977-8ed6-4081-af65-59c70db2dbfb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your fine-tuned model is available here:\n",
      "\n",
      "s3://this.output.should.be.replaced.with.a.real.s3.path.once.the.cell.is.executed/\n"
     ]
    }
   ],
   "source": [
    "# Show where the fine-tuned model is stored - previous job must be 'Completed' before running this cell\n",
    "model_archive_path = pt_estimator.jobs[-1].describe()['ModelArtifacts']['S3ModelArtifacts']\n",
    "print(f\"Your fine-tuned model is available here:\\n\\n{model_archive_path}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68f529f-a548-4fbd-b160-3cab5f52c488",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "**Note:** Please copy the above S3 path, as it will be required in the subsequent workshop module.\n",
    "\n",
    "\n",
    "Lastly, run the following cell to list the model artifacts available in your S3 model_archive_path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27ad8c7e-6a73-4f20-944f-ac12ef286a6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-13 01:01:39        714 config.json\n",
      "2025-05-13 01:01:48        124 generation_config.json\n",
      "2025-05-13 01:01:40 4400216536 model.safetensors\n",
      "2025-05-13 01:01:47        551 special_tokens_map.json\n",
      "2025-05-13 01:01:47    1842795 tokenizer.json\n",
      "2025-05-13 01:01:39     499723 tokenizer.model\n",
      "2025-05-13 01:01:48       1368 tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "# View the contents of the fine-tuned model path in S3\n",
    "!aws s3 ls {model_archive_path}/merged_model/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca9ffa7-a694-48c0-acde-cd468d18a448",
   "metadata": {},
   "source": [
    "Congratulations on completing the LLM fine-tuning module!\n",
    "\n",
    "In the next notebook, you will learn how to deploy your fine-tuned model in a SageMaker hosted endpoint, and leverage AWS Inferentia accelerators to perform model inference. Have fun!"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "aws_neuronx_venv_pytorch_latest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
